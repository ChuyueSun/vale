include "../../../arch/x64/X64.Vale.InsBasic.vaf"
include "../../../arch/x64/X64.Vale.InsMem.vaf"
include "../../../arch/x64/X64.Vale.InsVector.vaf"
include "X64.AES.vaf"
include "X64.GF128_Mul.vaf"
include "X64.GCTR.vaf"
include "X64.GHash.vaf"
include "X64.GCMencrypt.vaf"

module X64.GCMdecrypt

#verbatim{:interface}{:implementation}
open FStar.Seq
open Words_s
open Words.Seq_s
open Types_s
open Types_i
open AES_s
open GCTR_s
open GCTR_i
open GCM_i
open GHash_s
open GHash_i
open GCM_s
open X64.AES
open GF128_s
open GF128_i
open X64.Poly1305.Math_i
open GCM_helpers_i
open X64.GHash
open X64.GCTR
open X64.Machine_s
open X64.Vale.State_i
open X64.Vale.Decls_i
open X64.Vale.InsBasic
open X64.Vale.InsMem
open X64.Vale.InsVector
open X64.Vale.InsAes
open X64.Vale.QuickCode_i
open X64.Vale.QuickCodes_i
open X64.GF128_Mul
open X64.GCMencrypt
#endverbatim

#verbatim{:interface}

let nat32_workaround (i:int) : nat32 = if 0 <= i && i < pow2_32 then i else 0

let make_ghash_plain_LE (s:seq quad32) : ghash_plain_LE = if length s = 0 then create 1 (Mkfour 0 0 0 0) else s

#endverbatim


///////////////////////////
// GCM
///////////////////////////

#reset-options "--z3rlimit 30"
procedure {:quick} gcm_decrypt_stdcall_inner_inner(
    inline alg:algorithm,
    ghost args_b:buffer64,
    ghost iv_BE:quad32,

    ghost plain_b:buffer128,
    ghost auth_b:buffer128,
    ghost iv_b:buffer128,
    ghost out_b:buffer128,
    ghost tag_b:buffer128,

    ghost key:aes_key_LE(alg),
    ghost round_keys:seq(quad32),
    ghost keys_b:buffer128
    )
    modifies
        rax; rbx; rcx; rdx; rsi; r8; r9; r10; r11; r12; r13; r14; r15; xmm0; xmm1; xmm2; xmm3; xmm4; xmm5; xmm6; xmm7; xmm8; xmm10; xmm11; 
        mem; efl; 
    lets
        args_ptr := r9;

        plain_ptr         := buffer64_read(args_b, 0, mem);
        plain_num_bytes   := buffer64_read(args_b, 1, mem);
        auth_ptr          := buffer64_read(args_b, 2, mem);
        auth_num_bytes    := buffer64_read(args_b, 3, mem);
        iv_ptr            := buffer64_read(args_b, 4, mem);
        expanded_key_ptr  := buffer64_read(args_b, 5, mem);
        out_ptr           := buffer64_read(args_b, 6, mem);
        tag_ptr           := buffer64_read(args_b, 7, mem);

    requires
        validSrcAddrs64(mem, args_ptr, args_b, 8);

        buffers_disjoint128(plain_b, out_b);
        buffers_disjoint128(auth_b, out_b);
        buffers_disjoint128(keys_b, out_b);
        buffers_disjoint128(out_b, tag_b);
        
        validSrcAddrs128(mem, plain_ptr, plain_b, bytes_to_quad_size(plain_num_bytes));
        validSrcAddrs128(mem, auth_ptr, auth_b, bytes_to_quad_size(auth_num_bytes));
        validSrcAddrs128(mem, iv_ptr, iv_b, 1);
        validDstAddrs128(mem, out_ptr, out_b, bytes_to_quad_size(plain_num_bytes));
        validDstAddrs128(mem, tag_ptr, tag_b, 1);

        plain_ptr + 16 * bytes_to_quad_size(plain_num_bytes) < pow2_64;
        auth_ptr  + 16 * bytes_to_quad_size(auth_num_bytes)  < pow2_64;
        out_ptr   + 16 * bytes_to_quad_size(plain_num_bytes) < pow2_64;
        buffer_length(plain_b) == buffer_length(out_b);
        buffer_length(out_b) == bytes_to_quad_size(plain_num_bytes);
        buffer_length(auth_b) == bytes_to_quad_size(auth_num_bytes);
        256 * buffer_length(plain_b) < pow2_32;
        4096 * plain_num_bytes < pow2_32;
        4096 * auth_num_bytes < pow2_32;

        iv_BE == reverse_bytes_quad32(buffer128_read(iv_b, 0, mem));

        // To simplify length calculations, restrict auth and plain length further
        256 * bytes_to_quad_size(auth_num_bytes)  < pow2_32;
        256 * bytes_to_quad_size(plain_num_bytes) < pow2_32;

        // AES reqs
        alg = AES_128 || alg = AES_256;
        length(round_keys) == nr(alg) + 1;
        round_keys == key_to_round_keys_LE(alg, key);
        validSrcAddrs128(mem, expanded_key_ptr, keys_b, nr(alg) + 1);
        buffer128_as_seq(mem, keys_b) == round_keys;

    ensures
        modifies_buffer128(out_b, old(mem), mem);

        validSrcAddrs128(mem, old(out_ptr), out_b, bytes_to_quad_size(old(plain_num_bytes)));
        //validSrcAddrs128(mem, old(tag_ptr), tag_b, 1);

        let auth   := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(old(mem), auth_b)),  old(auth_num_bytes));
        let plain  := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(old(mem), plain_b)), old(plain_num_bytes));
        let cipher := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(mem,      out_b)),   old(plain_num_bytes));

//        // TODO: First two clauses work around Vale's type limitations
        4096 * length(plain) < pow2_32 /\ 
        4096 * length(auth) < pow2_32 /\
        cipher == fst(gcm_decrypt_LE(alg, seq_nat32_to_seq_nat8_LE(key), be_quad32_to_bytes(iv_BE), plain, auth, le_quad32_to_bytes(buffer128_read(tag_b, 0, old(mem))))) /\
        le_quad32_to_bytes(xmm1) == snd(gcm_encrypt_LE(alg, seq_nat32_to_seq_nat8_LE(key), be_quad32_to_bytes(iv_BE), plain, auth));
        r15 == old(buffer64_read(args_b, 7, mem));
{
    Load64_buffer(r14, r9,  0, args_b, 0);
    Load64_buffer(r13, r9,  8, args_b, 1);
    Load64_buffer(rax, r9, 16, args_b, 2);
    Load64_buffer(r11, r9, 24, args_b, 3);
    Load64_buffer(r10, r9, 32, args_b, 4);
    Load64_buffer(r8,  r9, 40, args_b, 5);
    Load64_buffer(rbx, r9, 48, args_b, 6);
    Load64_buffer(r15, r9, 56, args_b, 7);

    // Load the IV into its XMM register
    Load128_buffer(xmm7, r10, 0, iv_b, 0);

    gcm_core(alg, iv_BE, plain_b, auth_b, out_b, key, round_keys, keys_b);
}



#reset-options "--z3rlimit 20"
procedure {:quick} gcm_decrypt_stdcall_inner(
    inline alg:algorithm,
    ghost args_b:buffer64,
    ghost iv_BE:quad32,

    ghost plain_b:buffer128,
    ghost auth_b:buffer128,
    ghost iv_b:buffer128,
    ghost out_b:buffer128,
    ghost tag_b:buffer128,

    ghost key:aes_key_LE(alg),
    ghost round_keys:seq(quad32),
    ghost keys_b:buffer128
    )
    modifies
        rax; rbx; rcx; rdx; rsi; r8; r9; r10; r11; r12; r13; r14; r15; xmm0; xmm1; xmm2; xmm3; xmm4; xmm5; xmm6; xmm7; xmm8; xmm10; xmm11; 
        mem; efl; 
    lets
        args_ptr := r9;

        plain_ptr         := buffer64_read(args_b, 0, mem);
        plain_num_bytes   := buffer64_read(args_b, 1, mem);
        auth_ptr          := buffer64_read(args_b, 2, mem);
        auth_num_bytes    := buffer64_read(args_b, 3, mem);
        iv_ptr            := buffer64_read(args_b, 4, mem);
        expanded_key_ptr  := buffer64_read(args_b, 5, mem);
        out_ptr           := buffer64_read(args_b, 6, mem);
        tag_ptr           := buffer64_read(args_b, 7, mem);

    requires
        validSrcAddrs64(mem, args_ptr, args_b, 8);

        buffers_disjoint128(plain_b, out_b);
        buffers_disjoint128(auth_b, out_b);
        buffers_disjoint128(keys_b, out_b);
        buffers_disjoint128(out_b, tag_b);
        
        validSrcAddrs128(mem, plain_ptr, plain_b, bytes_to_quad_size(plain_num_bytes));
        validSrcAddrs128(mem, auth_ptr, auth_b, bytes_to_quad_size(auth_num_bytes));
        validSrcAddrs128(mem, iv_ptr, iv_b, 1);
        validDstAddrs128(mem, out_ptr, out_b, bytes_to_quad_size(plain_num_bytes));
        validDstAddrs128(mem, tag_ptr, tag_b, 1);

        plain_ptr + 16 * bytes_to_quad_size(plain_num_bytes) < pow2_64;
        auth_ptr  + 16 * bytes_to_quad_size(auth_num_bytes)  < pow2_64;
        out_ptr   + 16 * bytes_to_quad_size(plain_num_bytes) < pow2_64;
        buffer_length(plain_b) == buffer_length(out_b);
        buffer_length(out_b) == bytes_to_quad_size(plain_num_bytes);
        buffer_length(auth_b) == bytes_to_quad_size(auth_num_bytes);
        256 * buffer_length(plain_b) < pow2_32;
        4096 * plain_num_bytes < pow2_32;
        4096 * auth_num_bytes < pow2_32;

        iv_BE == reverse_bytes_quad32(buffer128_read(iv_b, 0, mem));

        // To simplify length calculations, restrict auth and plain length further
        256 * bytes_to_quad_size(auth_num_bytes)  < pow2_32;
        256 * bytes_to_quad_size(plain_num_bytes) < pow2_32;

        // AES reqs
        alg = AES_128 || alg = AES_256;
        length(round_keys) == nr(alg) + 1;
        round_keys == key_to_round_keys_LE(alg, key);
        validSrcAddrs128(mem, expanded_key_ptr, keys_b, nr(alg) + 1);
        buffer128_as_seq(mem, keys_b) == round_keys;

    ensures
        modifies_buffer128(out_b, old(mem), mem);
        validSrcAddrs128(mem, old(out_ptr), out_b, bytes_to_quad_size(old(plain_num_bytes)));

        let auth   := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(old(mem), auth_b)),  old(auth_num_bytes));
        let plain  := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(old(mem), plain_b)), old(plain_num_bytes));
        let cipher := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(mem,      out_b)),   old(plain_num_bytes));

//        // TODO: First two clauses work around Vale's type limitations
        4096 * length(plain) < pow2_32 /\ 
        4096 * length(auth) < pow2_32 /\
        cipher == fst(gcm_decrypt_LE(alg, seq_nat32_to_seq_nat8_LE(key), be_quad32_to_bytes(iv_BE), plain, auth, le_quad32_to_bytes(buffer128_read(tag_b, 0, old(mem))))) /\
        (rax = 0) == snd(gcm_decrypt_LE(alg, seq_nat32_to_seq_nat8_LE(key), be_quad32_to_bytes(iv_BE), plain, auth, le_quad32_to_bytes(buffer128_read(tag_b, 0, old(mem)))));
{
    gcm_decrypt_stdcall_inner_inner(alg, args_b, iv_BE, plain_b, auth_b, iv_b, out_b, tag_b, key, round_keys, keys_b);
    
    assert validDstAddrs128(mem, r15, tag_b, 1);

    // Auth tag is in xmm1; compare it to the value in memory
    Load128_buffer(xmm0, r15, 0, tag_b, 0);
    assert xmm0 == buffer128_read(tag_b, 0, mem);       // OBSERVE?
    assert xmm0 == buffer128_read(tag_b, 0, old(mem));  // OBSERVE?
    ghost var alleged_tag_quad := xmm0;
    ghost var computed_tag := xmm1;

    XmmEqual();
    ghost var auth   := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(old(mem), auth_b)),  old(auth_num_bytes));
    ghost var plain  := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(old(mem), plain_b)), old(plain_num_bytes));
    decrypt_helper(alg, seq_nat32_to_seq_nat8_LE(key), be_quad32_to_bytes(iv_BE), plain, auth, rax, alleged_tag_quad, computed_tag); 
}

#reset-options "--z3rlimit 400"
procedure {:quick} gcm_decrypt_stdcall(
    inline win:bool,
    inline alg:algorithm,
    ghost args_b:buffer64,
    ghost iv_BE:quad32,

    ghost plain_b:buffer128,
    ghost auth_b:buffer128,
    ghost iv_b:buffer128,
    ghost out_b:buffer128,
    ghost tag_b:buffer128,
    ghost stack_b:buffer64,

    ghost key:aes_key_LE(alg),
    ghost round_keys:seq(quad32),
    ghost keys_b:buffer128
    )
    modifies
        rax; rbx; rcx; rdx; r8; r9; r10; r11; r12; r13; r14; r15; xmm0; xmm1; xmm2; xmm3; xmm4; xmm5; xmm6; xmm7; xmm8; xmm10; xmm11; 
        rbp; rdi; rsi; 
        mem; efl; rsp;
    lets
        args_ptr := if win then rcx else rdi;

        plain_ptr         := buffer64_read(args_b, 0, mem);
        plain_num_bytes   := buffer64_read(args_b, 1, mem);
        auth_ptr          := buffer64_read(args_b, 2, mem);
        auth_num_bytes    := buffer64_read(args_b, 3, mem);
        iv_ptr            := buffer64_read(args_b, 4, mem);
        expanded_key_ptr  := buffer64_read(args_b, 5, mem);
        out_ptr           := buffer64_read(args_b, 6, mem);
        tag_ptr           := buffer64_read(args_b, 7, mem);

    requires
        validSrcAddrs64(mem, args_ptr, args_b, 8);

        buffers_disjoint128(plain_b, out_b);
        buffers_disjoint128(auth_b, out_b);
        buffers_disjoint128(keys_b, out_b);
        buffers_disjoint128(out_b, tag_b);
        
        buffers_disjoint(args_b, stack_b);

        locs_disjoint(list(loc_buffer(stack_b), loc_buffer(plain_b)));
        locs_disjoint(list(loc_buffer(stack_b), loc_buffer(auth_b)));
        locs_disjoint(list(loc_buffer(stack_b), loc_buffer(iv_b)));
        locs_disjoint(list(loc_buffer(stack_b), loc_buffer(out_b)));
        locs_disjoint(list(loc_buffer(stack_b), loc_buffer(tag_b)));
        locs_disjoint(list(loc_buffer(stack_b), loc_buffer(keys_b)));

        validDstAddrs128(mem, plain_ptr, plain_b, bytes_to_quad_size(plain_num_bytes));
        validSrcAddrs128(mem, auth_ptr, auth_b, bytes_to_quad_size(auth_num_bytes));
        validSrcAddrs128(mem, iv_ptr, iv_b, 1);
        validSrcAddrs128(mem, out_ptr, out_b, bytes_to_quad_size(plain_num_bytes));
        validSrcAddrs128(mem, tag_ptr, tag_b, 1);

        valid_stack_slots(mem, rsp, stack_b, if win then 18 else 8);

        plain_ptr + 16 * bytes_to_quad_size(plain_num_bytes) < pow2_64;
        auth_ptr  + 16 * bytes_to_quad_size(auth_num_bytes)  < pow2_64;
        out_ptr   + 16 * bytes_to_quad_size(plain_num_bytes) < pow2_64;
        buffer_length(plain_b) == buffer_length(out_b);
        buffer_length(out_b) == bytes_to_quad_size(plain_num_bytes);
        buffer_length(auth_b) == bytes_to_quad_size(auth_num_bytes);
        256 * buffer_length(plain_b) < pow2_32;
        4096 * plain_num_bytes < pow2_32;
        4096 * auth_num_bytes < pow2_32;

        iv_BE == reverse_bytes_quad32(buffer128_read(iv_b, 0, mem));

        // To simplify length calculations, restrict auth and plain length further
        256 * bytes_to_quad_size(auth_num_bytes)  < pow2_32;
        256 * bytes_to_quad_size(plain_num_bytes) < pow2_32;

        // AES reqs
        alg = AES_128 || alg = AES_256;
        length(round_keys) == nr(alg) + 1;
        round_keys == key_to_round_keys_LE(alg, key);
        validSrcAddrs128(mem, expanded_key_ptr, keys_b, nr(alg) + 1);
        buffer128_as_seq(mem, keys_b) == round_keys;

    ensures
        modifies_mem(loc_union(loc_buffer(stack_b), loc_buffer(out_b)), old(mem), mem);

        validSrcAddrs128(mem, old(out_ptr), out_b, bytes_to_quad_size(plain_num_bytes));

        let auth   := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(old(mem), auth_b)),  old(auth_num_bytes));
        let plain  := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(old(mem), plain_b)), old(plain_num_bytes));
        let cipher := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(mem,      out_b)),   old(plain_num_bytes));

//        // TODO: First two clauses work around Vale's type limitations
        4096 * length(plain) < pow2_32 /\ 
        4096 * length(auth) < pow2_32 /\
        cipher == fst(gcm_decrypt_LE(alg, seq_nat32_to_seq_nat8_LE(key), be_quad32_to_bytes(iv_BE), plain, auth, le_quad32_to_bytes(buffer128_read(tag_b, 0, old(mem))))) /\
        (rax = 0) == snd(gcm_decrypt_LE(alg, seq_nat32_to_seq_nat8_LE(key), be_quad32_to_bytes(iv_BE), plain, auth, le_quad32_to_bytes(buffer128_read(tag_b, 0, old(mem)))));

        // Calling convention for caller/callee saved registers
        rsp == old(rsp);

        // Windows:
        win ==> rbx == old(rbx);
        win ==> rbp == old(rbp);
        win ==> rdi == old(rdi);
        win ==> rsi == old(rsi);
        win ==> r12 == old(r12);
        win ==> r13 == old(r13);
        win ==> r14 == old(r14);
        win ==> r15 == old(r15);

        win ==> xmm6  == old(xmm6);
        win ==> xmm7  == old(xmm7);
        win ==> xmm8  == old(xmm8);
        win ==> xmm10 == old(xmm10);
        win ==> xmm11 == old(xmm11);

        // Linux:
        !win ==> rbx == old(rbx);
        !win ==> rbp == old(rbp);
        !win ==> r12 == old(r12);
        !win ==> r13 == old(r13);
        !win ==> r14 == old(r14);
        !win ==> r15 == old(r15);
{
    // Shuffle the incoming pointer around
    inline if (win) {
        Mov64(r9, rcx);
    } else {
        Mov64(r9, rdi);
    }

    callee_save_registers(win, stack_b);
    gcm_decrypt_stdcall_inner(alg, args_b, iv_BE, plain_b, auth_b, iv_b, out_b, tag_b, key, round_keys, keys_b);
    Mov64(rdx, rax);        // Save a copy of rax == result
    callee_restore_registers(win, stack_b, old(xmm6), old(xmm7), old(xmm8), old(xmm10), old(xmm11));
    Mov64(rax, rdx);
}
